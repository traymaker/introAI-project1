\documentclass[a4paper,12pt]{article}
\usepackage{enumitem}
\renewcommand{\thesubsection}{\thesection\alph{subsection}}
\begin{document}

\title{Intro to AI Project 1}
\author{Tim Raymaker and Mike Rizzo}
\date{\today}
\maketitle
\section{Part 1: Understanding the Methods}
\subsection{}
The agent will move toward the cell with the lowest f-value. The f-value is determined from the sum of the cost from the start, or g-value,  and the heuristic value to the goal, or h-value. For the first move, each possible direction is one tile away from the start, meaning that the g-value for each move will be the same, and differences in the h-value will be the deciding factor on where to move. Since the agent does not know that cells D3 and E4 are blocked, it will move to the east, the direction that leads to the cell with the lowest h-value and therefore the lowest f-value. 
\subsection{}
The agent will continue to act as long as the open list remains populated. The open list is created from cells adjacent to expanded cells. This means that any cells that is adjacent to a reachable cell will be expanded eventually, until the target is reached. Therefore, if an unblocked path from the start state to the end state exists, the agent will eventually find it, even if it has to expand every possible cell. The grid worlds are finite in size and so there is a finite number of cells that will be added to be expanded. Expansion can be done in a finite amount of time and traversal can be done in a finite amount of time. Considering both processes can be done in finite time and there are only finite cells to iterate over, in the scenario given a path can be found in finite time. 
\begin{enumerate}
	\item Case 1: 
	\item Case 2:
	\item Case 3:
	\item Case4:
\end{enumerate}
\section{Part 2}
Data was collected for Repeat Forward A* with Tie-breaking in favor of Low $g$-values, Repeat Forward A* with Tie-breaking in favor of High $g$-values,  Repeat Backward A* with Tie-breaking in favor of Low  $g$-values, Repeat Backward A* with Tie-breaking in favor of High $g$-values, Adaptive A* with Tie-breaking in favor of High $g$-values. The searches were each performed on the 50 maps generated in Part 0. The start and end goal were standardized to $(0,0)$ and $(100,100)$. Of the 50, 26 had solutions. The 24 without solutions are marked in red and their runtimes are not included in the data because animation conflicted with reporting significant and comparable runtimes. The values are recorded in the figure below

%figure 

We observed for  that favoring a high or low $g$ in the case of a tie value had little to no effect on the performance of the Repeated Forward A* search. On average, the runtime for favoring low $g$ was ~5.84 seconds. Favoring high $g$ was nearly identical, being slower on average only by a hundredth of a second at ~5.86 seconds. Investigating further, we see the median Though our runtimes were nearly identical, using g to tiebreak would likely be useful for performance on an empty map as it would help limit expansions. In the maps provided, the amount of obstacles in the way means that there is relatively little gain with manipulating $g$. 	
\section{Part 3}
The Repeated Forward A* search and Repeated Backward A* search were both able to find possible solutions, where solutions existed, but their exact paths differed in most cases. The difference comes from the difference in path planning and replanning. Because backward plans from the goal, different blocked and unblocked cells are found at each iteration. Neither methodâ€™s path was consistently superior to that of the other. 
\section{Part 4}
\section{Part 5}
\section{Part 6}
Changing the method of visualizing the path of the agent would likely yield the greatest reduction in time consumption. 
\newline
(1,001 x 1,001) = 1002001 cells \newline
At 2 bits per cell: \newline
2 * 1002001  = 2,004,002 bits ~ 250 kB \newline \newline

4MB = 32000000 bits \newline
At 2 bits per cell: \newline
	16000000 cells possible \newline 
	grid size = sqrt(16000000) = 4000 x 4000 grid
\end{document}
